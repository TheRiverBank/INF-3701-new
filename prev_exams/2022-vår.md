# Task 1
## a) Which are the challenges of modern data management applications compared to traditional applications of relational database? Describe the Big Data challenges and the main properties of Big Data.
Modern databases must take in to account a much wider range of data representations as well as greatly increased data sizes and the demand for quick access and retreival. While traditional applications of databases might only require simple operations in a smaller business setting, modern applications like social media applications or machine learning models have quite different needs in terms of performance and variaty of data and how it should be presented to be best utalized.

There are many *V's* that are assosiated with big data but most improtant are variety, velocity and volume. These are also the challanges that big data systems face in order to support online analytics processes and online transaction processes that wants to visualize and do operations on large amounts of data quickly. One of the main challanges of big data is that the data comes form many different sources. This requires the system to create a unifed view of the data when it is possible that the data sources has quire different schemas. One approach is to try to reconcile these schemas into a data warehouse, or create a big data lake of unstructured data. There are much deeper apporaches that also has to be evaluated like wheter to use a global as view approach or a local as view approach. It must also be considered how this data is to be stored. So for instance in a centralized database, a parallel database or a distributed database which all comes with different strengths and weaknesess. 

## b) Describe what is OLAP, what is a data cube, and which are the 4 main operations within OLAP systems.
OLAP stands for Online Analytics Processing which is a system that lets users analyze large amounts of data in ways that will benefit a business thourgh increased revenue. This revenue might come from increased user insterest in ads via personalization or finding which wares are in most demand in a store. These "trends" can be found thorugh machine learning alogorithms like regression, SVM, decision trees or neural networks. 

A data cube is a representation of the data in a table that is modeled like a cube. For example using three dimensions, the cube axes could be clothing type, size and color. The mentioned attributes are dimension attributes and the values within this cube are measure attributes like for example number of items sold for that particular item, color and size. The four operations within OLAP systems that relate to the data cube are:
* Rollup, which is to look at the data in a coarser aggregation level
* Drill down, which is to look at the data in a finer aggregation level
* Pivot, which is to switch the current cross-tab. A cross-tab is a two dimentional representation of some dimention in the data cube, like just color and clothing type. 
* Slicing, which is to look at for example clothing type and size for a fixed color. This can also be viewed as a dice which is why slicing can also be called dicing.

# Task 2
## a) Describe the classification hierarchy as a directed acyclic graph (DAG) within information retrieval systems. Describe what a directory is in the context of information retrieval systems. 
A classification hierarchy as a DAG is a composition of nodes and edges that explains class relationships between the nodes. For instance, books can be sub-divided into books about computer science and math, which again can be divided into specific topics. It is possible that topics are in common to both computer science and math and thus both nodes might have an edge to the same topic. In information retreival system, the directory is one such DAG classification structure where the nodes, particulary the leaf nodes, has links to the documents for a particular class. This is a method of trying to origanize information and makes finding classes easier by allowing for traversal from the root node to the desired topic. 

## b) Describe the challenges of false negatives and false positives occurring in document ranking strategies. Describe the two metrics to measure retrieval effectiveness, precision and recall.
False negatives occur when a document that is in fact relevant to the user is classified as not relevant and false positives occur when a document that is not relevant to the user is classified as relevant. Challange that occur in information retreival systems are that keywords used in queries can have synonymes and homonymes which makes it hard to fetch all relevant documents. We would like to use synonymes as the user would want documents even though they dont use the exact wording (use "or" for each sysnomym). The problem is homonymes which can retreive different documents than the one asked for which synonymes further problmatezises. There might be documents that does not contain any of the keywords used but is still in fact relevant to the user but which the infomration retreival system failed to interpret the semanticts of the query, document or both. On the other hand, some documents might inject many random keywords just to have a higher chance of being displayed to the user but which has no value to the user and thus causing a false positive. Worse still is that such sites can be used for mallicious purposes which is not good for the infomration retreival system. 

# Task 3
## a) Discuss the difference(s) between providing replication consistency and database consistency.
In a distributed system, some nodes might be used for replication in order to improve fault tolerance in the system. There can then be some primary node that get an update request who susequently propagates this update to the replicas who also performs the update. The replicas are usually in different geographical areas to account for local failures. There are different types of replication consistency apporaches, for instance strict consistency where the system makes sure the replication sucessfully completes before continuing. There is also weak or eventual consistency where the system makes sure that the update is eventually propagated to the other replicas. There is no best solution but depends on the requirements of the user in terms of consistency level and availability. 

Database consistency on the other hand is one of the ACID (atomicity, consistency, isolation and durability) properties defined in DBMS's which is directed more finely towards individual updates, checking that they are legal and that the update achieves the desired outcome. This can be done by setting integrity constraints such as dunctional dependancies, check constraints, assertions or triggers or primary-key constraints. In addition, the database should be in a consistent state before and after the update occurs. This could for instance be in a bank application where user A sends 50$ to user B. User A's account should then have been withdrawn 50$ and user B should have received 50$ and the total sum of their balance should be the same before and after the transaction. Dedicated logic has to be implemented to make sure the transactions are consistent but also be able to roll back if a consistency requirement is violated by the update.

So in essence, the difference is that replication consistency handles consistency of replicated data across replication nodes while database consistency handles the consistency of individual updates to a single database. They can of course be used in combination where each replication evaluates its database consistency before and after replicating some data item. 

## b) Describe how the quorum consensus protocol can be used to express both strict and weak consistency. Explain the consequences of allowing weak consistency. 
Consensus protocols are used in distributed database systems to make sure that the nodes in the system are in an agreement on whether or not, or how to execute some read or write. Quorum consensus basically means how many participants are needed to execute some operation. This relates to the trade off between consistency and availability especially when cosidering the CAP theorem which states that one can have either consistency or availability in the face of network paritions. Strict consistency requires that the quorum consist of a larger portion of the nodes in the network, or possibly all nodes in the network. This makes sure that a read or write is seen by most/all nodes and thus enables the network to give the user the most up to date data item or makes sure that an update is properly replicated before the system can continue on new transactions. This decreases the availability of the system but increases consistency. In weak consistency, a smaller quorum can be defines which allows reads or writes to be completed more rapidly but must rely on eventual consistency and the user might read out of date data. A system that is able to tune the consistency constraints is Cassandra where the user who implements the system can define the consistency strictness. 

Systems that benefit from strict consistency are those where it is critical that an update is completed in its entirety, also at the replicas, before the system may continue. This could be for example a banking system. A system that can benefit from weak consistency is perhaps a social media application where it is not crucial that for example a comment or like is eventually brought to a consistent state within the system.

# Task 4
## a) Describe objective(s) and challenge(s) of data partitioning in homogeneous distributed databases.
Data partition is done particually in paralell database system where the goal is to take advatage of some inherit paralellism in the proessing of the data. In this way, the processing can be done in paralell before for example the result of all individual processing is aggregated. This can speed up processing substastially but communication overhead should be considered, although paralell systems are often located in the same rack or in the same datacenter. Partitioning can also be done in order to distribute data such that data that is frequently used by some region in the world lies close to them. This could for example be that .no websites are close to users in Norway but much less to to the users in Japan. 

Since the databases are homoheneous, there should not be any problems of assigning which node does how much work in a paralell processing task. The challanges can be how to determine how the data is going to be used which affects the paritioning strategy as there are several (round-robin, range, hash). Different uses cases might be analytics, machine learning or storing social media data. Using a range partition, the databases are going to be primed for range based queries and the management system can easily find the partitions with the requested values. A drawback to this is that data or processing skew can occur if there are more values in some ranges than there are in others. A way to solve this might be to introduce vitrual nodes. Using a hash based approach like consistent hashing, it is easier to assure that the data spread is more even but it does not by it self take into account things like temporal properties that might be in the data. It also doesnt take into account the differences in requested content in different geographic areas as noted eariler. These are some of the things the implementor must consider and solve before deploting the distributed system. 

Another thing to consider is what portions of the data is most frequently used. One approach is to vertically partition tables such complete tuples are spread across the databases. Another is to distribute the columns, assuming that some columns are going to be used more frequently than others which increases processing speed as whole tuples dont have to be read if only a small portion of it is going to be used. 

## b) Describe how semi-structured data can be used to overcome interoperability problems.
interoperability = in a query, divide the operations in the query such that the query can be processed in parallel. 

semi-structured data = ex wide column or sparse column

Interoperability problems are those where there are data coming from mutiple different sources that has to be processed and possibly unified. Semi-structured data can overcome problems that arrise in such situations as semi-structured data stores are not strict on how the data must be represented in the database and thus removes the problem of creating some global conceptual schema that caters to all the different data streams. Like in a wide column approach, each row may have its own set of columns as well as having a different number of columns than the other rows. The data can then be directly inserted into the wide column store without modifying a potential schema. 